---
title: "Course Project - Practical Machine Learning"
author: "gsgxnet"
date: "23 April 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ML - Course Project


## Background

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

### Data

#### The training data for this project is sourced from:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

#### And the test data from:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The original source for this data this project comes from this source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har). We are allowed to use it in our documents, so we are happy to cite them. We regard them allowing their data for free use as being very generous.

## Data Processing

### Get data 
```{r getdata}
library(utils)
if(!file.exists("pml-training.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  destfile = "pml-training.csv", method = "curl", quiet = TRUE)
}
if(!file.exists("pml-testing.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                  destfile = "pml-testing.csv", method = "curl", quiet = TRUE)
}
```

The original dataset can be downloaded [from](http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv) 

The content of the database is further described by Groupware@LES [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises). 

the models we will use are very computing intensive, so we boost computing performance by parallel processing:
```{r, message=FALSE}
# configure multicore
library(doMC)
registerDoMC(cores=4)
```


## Loading and preprocessing the data

```{r }
library(data.table)
pmltrain <- read.csv("pml-training.csv")
pmlcols <- colnames(pmltrain)
pmltest <- read.csv("pml-testing.csv")
```

Looking at the tables shows 2 problems:

1. the testing dataset contains only 60 columns with data, the other columns contain either all NA or are blank. 

2. in the training set there are 2 kinds of rows - normal ones like those in the testing set and others with a very different content, not similar to any row in the testing set. 

"bad" records look like:

"9276","carlitos",1323084280,984287,"05/12/2011 11:24","yes",857,1.59,6.04,-92.8,3,"0.306838","#DIV/0!","#DIV/0!","1.515258","#DIV/0!","#DIV/0!",-92.7,3,"0.3",-92.8,3,"0.3",0.1,0,"0.00",0,1.459,0.2084, ... , "B"

These might be some kind of aggregations of other records. But as they have no resemblence in the testing set, they should be regarded like outliers and eliminated.

Another reason to dismiss these records are the "#DIV/0!" values within some of their columns. They are not interpreteable at all.

Removing empty columns from the testing set:

```{r}
pmltestNNA <- Filter(function(x)!all(is.na(x)), pmltest)
colNNA <- colnames(pmltestNNA)
```

eliminating the "bad" records and empty columns from the training set:

```{r}
pmltrainX <- pmltrain[!complete.cases(pmltrain),]  # bad records
pmltrainXNNA <- Filter(function(x)!all(is.na(x)), pmltrainX)  # col NA only
pmltrainXNNAe <- Filter(function(x)!all((x == "")), pmltrainXNNA)  # empty col
colXNNAe <- colnames(pmltrainXNNAe)  # usable cols in training 
coltesttrainequal <- colNNA == colXNNAe  # compare colnames in test and training 
which(!coltesttrainequal)
```

After these cleaning steps, both sets have the same columns, besides column 60 which contains the classe in the training set and the problem_id in the testing set.


## Classification Models

First we fit a random forest model thru the caret library. To make it a reliable model we include a 10-fold cross validation. 

```{r, message=FALSE}
library(caret)
contrlcv <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)
modfitrf <- train(x= pmltrainXNNAe[,8:59], y=pmltrainXNNAe[,60],
                  method="rf",data=pmltrainXNNAe,trControl = contrlcv)

```

To look at a summary of the model's performance, a print of the model is the easiest way:

```{r}
getTrainPerf(modfitrf)
```

We get a very reliable model with an out of sample accuray of 99.5%. 


```{r}
predtestrf <- predict(modfitrf, pmltestNNA[,8:59])
predtestrf
```

Next we compare the random forest model to C5.0 tree:

```{r, message=FALSE, warning=FALSE}
grid_c50 <- expand.grid(.model = "tree",
                          .trials = c(10, 20, 30, 40),
                          .winnow = "FALSE")
modfitc50 <- train(x= pmltrainXNNAe[,8:59], y=pmltrainXNNAe[,60], method = "C5.0",
                 metric = "Kappa", trControl = contrlcv,
                 tuneGrid = grid_c50)
getTrainPerf(modfitc50)
```

this C5.0 tree model is even more reliable than the random forest.
In detail:

```{r}
cvVal <- resamples(list(C50 = modfitc50, rf = modfitrf))
dotplot(cvVal, metric = "Kappa")
```

There is no sign of overfitting.

```{r}
predtestc50 <- predict(modfitrf, pmltestNNA[,8:59])
```

Comparing the prediction for the test set by the C5.0 model with the rf model:

```{r}
modeq <- predtestc50 == predtestrf
which(!modeq)
```

Both models agree in the predictions for all cases of the testing set. 
So there could be no gain at all in continuing with a further kind of prediction model, if we add it to an ensemble it would be overvoted by these two already agreeing models.

## Conclusion

We got two very reliable models for prediction. 
Uploading the prediction results to the course project quiz showed 100% accuracy of them. 


## Remarks

Some of the used methods are not covered by the course material. These are described in the book:
[Machine Learning with R](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-r), Brett Lantz - Packt Publishing, October 2013
